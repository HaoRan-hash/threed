{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from ignite.metrics import Accuracy\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 数据部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeNet(Dataset):\n",
    "    def __init__(self, root, split, npoints=1024):\n",
    "        super(ShapeNet, self).__init__()\n",
    "        self.npoints = npoints\n",
    "        self.idx_to_class = {}\n",
    "        dir_to_idx = {}\n",
    "\n",
    "        with open(os.path.join(root, 'synsetoffset2category.txt'), 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip().split()\n",
    "                self.idx_to_class[i] = line[0]\n",
    "                dir_to_idx[line[1]] = i\n",
    "        \n",
    "        self.files = []\n",
    "        self.object_labels = []\n",
    "        with open(os.path.join(root, 'train_test_split', f'shuffled_{split}_file_list.json'), 'r') as f:\n",
    "            temp = json.load(f)   # type(temp) = list\n",
    "\n",
    "        if split == 'train':\n",
    "            with open(os.path.join(root, 'train_test_split', 'shuffled_val_file_list.json'), 'r') as f:\n",
    "                temp += json.load(f)\n",
    "\n",
    "        for x in temp:\n",
    "            x = x.split('/')\n",
    "            self.files.append(os.path.join(root, x[1], x[2]+'.txt'))\n",
    "            self.object_labels.append(dir_to_idx[x[1]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def pcd_norm(self, points):\n",
    "        mean = points.mean(axis=0)\n",
    "        points = points - mean\n",
    "        max_dis = (np.sqrt((np.square(points)).sum(axis=1))).max()\n",
    "        points = points / max_dis\n",
    "\n",
    "        return points\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file = self.files[index]\n",
    "        temp = self.object_labels[index]\n",
    "\n",
    "        points = np.genfromtxt(file, dtype=np.float32)\n",
    "        choice = np.random.choice(len(points), self.npoints)\n",
    "        points = points[choice]\n",
    "\n",
    "        points[:, 0:3] = self.pcd_norm(points[:, 0:3])\n",
    "\n",
    "        part_label = points[:, -1].astype(np.int64)\n",
    "        points = points[:, 0:6]\n",
    "\n",
    "        object_label = np.zeros(16, dtype=np.float32)\n",
    "        object_label[temp] = 1\n",
    "\n",
    "        return points, object_label, part_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ShapeNet('shapenetcore_partanno_segmentation_benchmark_v0_normal', split='train', npoints=2048)\n",
    "test_dataset = ShapeNet('shapenetcore_partanno_segmentation_benchmark_v0_normal', split='test', npoints=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Airplane', 1: 'Bag', 2: 'Cap', 3: 'Car', 4: 'Chair', 5: 'Earphone', 6: 'Guitar', 7: 'Knife', 8: 'Lamp', 9: 'Laptop', 10: 'Motorbike', 11: 'Mug', 12: 'Pistol', 13: 'Rocket', 14: 'Skateboard', 15: 'Table'}\n",
      "14007\n",
      "2874\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.idx_to_class)\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2048, 6])\n",
      "torch.Size([16, 16])\n",
      "torch.Size([16, 2048])\n"
     ]
    }
   ],
   "source": [
    "a, b, c = next(iter(train_dataloader))\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 模型部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet(nn.Module):\n",
    "    def __init__(self, class_num):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.mlp_1 = nn.Sequential(nn.Conv1d(6, 64, kernel_size=1),\n",
    "                                    nn.BatchNorm1d(64),\n",
    "                                    nn.ReLU(inplace=True))\n",
    "        self.mlp_2 = nn.Sequential(nn.Conv1d(64, 128, kernel_size=1),\n",
    "                                    nn.BatchNorm1d(128),\n",
    "                                    nn.ReLU(inplace=True))\n",
    "        self.mlp_3 = nn.Sequential(nn.Conv1d(128, 128, kernel_size=1),\n",
    "                                    nn.BatchNorm1d(128),\n",
    "                                    nn.ReLU(inplace=True))\n",
    "        self.mlp_4 = nn.Sequential(nn.Conv1d(128, 512, kernel_size=1),\n",
    "                                    nn.BatchNorm1d(512),\n",
    "                                    nn.ReLU(inplace=True))\n",
    "        self.mlp_5 = nn.Sequential(nn.Conv1d(512, 2048, kernel_size=1),\n",
    "                                    nn.BatchNorm1d(2048))\n",
    "        self.mlp_6 = nn.Sequential(nn.Conv1d(4944, 256, kernel_size=1),\n",
    "                                    nn.BatchNorm1d(256),\n",
    "                                    nn.ReLU(inplace=True))\n",
    "        self.mlp_7 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=1),\n",
    "                                    nn.BatchNorm1d(256),\n",
    "                                    nn.ReLU(inplace=True))\n",
    "        self.mlp_8 = nn.Sequential(nn.Conv1d(256, 128, kernel_size=1),\n",
    "                                    nn.BatchNorm1d(128),\n",
    "                                    nn.ReLU(inplace=True))\n",
    "        self.mlp_9 = nn.Sequential(nn.Conv1d(128, class_num, kernel_size=1))\n",
    "        \n",
    "    def forward(self, points, object_labels):\n",
    "        \"\"\"\n",
    "        points.shape = (b, n, 6)\n",
    "        object_labels = (b, 16)\n",
    "        \"\"\"\n",
    "        _, n, _ = points.shape\n",
    "        points = points.permute(0, 2, 1)\n",
    "        object_labels = object_labels.unsqueeze(dim=2).repeat(1, 1, n)\n",
    "\n",
    "        out1 = self.mlp_1(points)\n",
    "        out2 = self.mlp_2(out1)\n",
    "        out3 = self.mlp_3(out2)\n",
    "        out4 = self.mlp_4(out3)\n",
    "        out5 = self.mlp_5(out4)\n",
    "\n",
    "        global_feature, _ = out5.max(dim=2, keepdim=True)\n",
    "        global_feature = global_feature.repeat(1, 1, n)\n",
    "\n",
    "        final_feature = torch.cat((out1, out2, out3, out4, out5, global_feature, object_labels), dim=1)\n",
    "        y = self.mlp_9(self.mlp_8(self.mlp_7(self.mlp_6(final_feature))))\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, metric_fn, optimizer, device, cur_epoch, total_epoch, show_gap, interval):\n",
    "    model.train()\n",
    "    if cur_epoch % show_gap == 0:\n",
    "        pbar = tqdm(dataloader, desc=f'Epoch {cur_epoch}/{total_epoch}', unit='batch')\n",
    "    else:\n",
    "        pbar = dataloader\n",
    "\n",
    "    for i, (x1, x2, y) in enumerate(pbar):\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x1, x2)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_fn.reset()\n",
    "        metric_fn.update((y_pred, y))\n",
    "        acc = metric_fn.compute()\n",
    "\n",
    "        if cur_epoch % show_gap == 0 and i % interval == 0:\n",
    "            pbar.set_postfix_str(f'loss={loss:.4f}, acc={acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_miou = 0\n",
    "best_epoch = 0\n",
    "\n",
    "object_to_part = {0: [0, 1, 2, 3], 1: [4, 5], 2: [6, 7], 3: [8, 9, 10, 11], 4: [12, 13, 14, 15], 5: [16, 17, 18], \n",
    "                    6: [19, 20, 21], 7: [22, 23], 8: [24, 25, 26, 27], 9: [28, 29], 10: [30, 31, 32, 33, 34, 35],\n",
    "                    11: [36, 37], 12: [38, 39, 40], 13: [41, 42, 43], 14: [44, 45, 46], 15: [47, 48, 49]}\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, device, cur_epoch, path, show_gap, log_dir):\n",
    "    model.eval()\n",
    "    steps = len(dataloader)\n",
    "    idx_to_class = dataloader.dataset.idx_to_class\n",
    "    loss = 0\n",
    "    object_mious = [[] for _ in range(16)]\n",
    "    logging.basicConfig(filename=log_dir, format='%(message)s', level=logging.INFO)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x1, x2, y in dataloader:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x1, x2)\n",
    "            loss += loss_fn(y_pred, y)\n",
    "\n",
    "            y_pred = y_pred.permute(0, 2, 1)\n",
    "            y_pred = F.softmax(y_pred, dim=-1)\n",
    "            for i in range(len(y_pred)):\n",
    "                cur_object_label = x2[i].argmax().item()\n",
    "                cur_y_pred = y_pred[i, :, object_to_part[cur_object_label]].argmax(dim=-1)\n",
    "                cur_y_pred += object_to_part[cur_object_label][0]\n",
    "                cur_y = y[i]\n",
    "                \n",
    "                temp = []\n",
    "                for part_class in object_to_part[cur_object_label]:\n",
    "                    if (torch.sum(cur_y == part_class) == 0 and torch.sum(cur_y_pred == part_class) == 0):\n",
    "                        temp.append(1)\n",
    "                    else:\n",
    "                        intersection = torch.sum((cur_y == part_class) & (cur_y_pred == part_class)).item()\n",
    "                        union = torch.sum((cur_y == part_class) | (cur_y_pred == part_class)).item()\n",
    "                        temp.append(intersection / union)\n",
    "                object_mious[cur_object_label].append(np.mean(temp))\n",
    "    \n",
    "    class_mious = [np.mean(object_mious[i]) for i in range(16)]\n",
    "    all_mious = [y for x in object_mious for y in x]\n",
    "    miou = np.mean(all_mious)\n",
    "            \n",
    "    loss = loss / steps\n",
    "\n",
    "    global best_miou, best_epoch\n",
    "    if miou >= best_miou:\n",
    "        torch.save(model.state_dict(), path)\n",
    "        best_miou = miou\n",
    "        best_epoch = cur_epoch\n",
    "\n",
    "    if cur_epoch % show_gap == 0:\n",
    "        logging.info(f'Epoch {cur_epoch}\\n')\n",
    "        for i in range(16):\n",
    "            logging.info(f'{idx_to_class[i]}:   {class_mious[i]:.4f}')\n",
    "        logging.info(f'test_loss={loss:.4f}, test_miou={miou:.4f}')\n",
    "        logging.info('-------------------------------------------------------')\n",
    "        print(f'test_loss={loss:.4f}, test_miou={miou:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "\n",
    "pointnet = PointNet(50).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "metric_acc = Accuracy(device=device)\n",
    "optimizer = torch.optim.Adam(pointnet.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 250\n",
    "show_gap = 1\n",
    "save_path = 'pointnet_partseg.pth'\n",
    "for i in range(epochs):\n",
    "    train_loop(train_dataloader, pointnet, loss_fn, metric_acc, optimizer, device, i, epochs, show_gap, 1)\n",
    "    test_loop(test_dataloader, pointnet, loss_fn, device, i, save_path, show_gap, 'pointnet_partseg.log')\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_miou, best_epoch)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10e950a2d02279e5457baa6feefd0499ef078fb1b2957debc5d17c17df001893"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch1.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
